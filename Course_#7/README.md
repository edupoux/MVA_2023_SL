# MVA. Algorithms for speech and natural language processing
# Course 3. Language Models

## Professors
E. Dupoux, N. Zeghidour, R. Riad

# Reading Materials

## Language modeling and decoding and learning

* Language models:  Jurasky & Martin (2017 Chapter 4; 2019 Chapter  3). Language modeling with N- grams https://web.stanford.edu/~jurafsky/slp3/3.pdf
*   Dynamic programming:  Jaehyun Park (2015). Course materials for CS 97SI, Stanford
University: http://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf
*   Viterbi decoding, Baum Welch:  Jurafsky & Martin (2019. Chapter 9/2019 Appendix A). Hidden Markov Models : https://web.stanford.edu/~jurafsky/slp3/A.pdf


## End-to-end speech recognition
* A pedagogical tutorial on CTC (Connectionist Temporal Classification): https://distill.pub/2017/ctc/
* Sequence-to-sequence for speech recognition: https://arxiv.org/abs/1508.01211

## to know more

* HMMs and Speech recognition: Gales & Young (2007). The application of Hidden Markow Models in Speech Recognition. Foundations and Trends in Signal Processing, 1(3), 195-304.

* First paper to introduce sequence-to-sequence: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf

* First paper to introduce attention in sequence-to-sequence: https://arxiv.org/abs/1409.0473

* background materials on WFSTs: http://www.openfst.org/twiki/bin/view/FST/FstBackground

# Practical Assignment

It is available as a python notebook here: TBA

Works on Chrome and Safari. Does not work on Mozilla

For more details, see this [page](../../../tree/master/TD_%231/).
